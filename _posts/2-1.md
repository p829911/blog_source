---
title: 2.1
date: 2019-11-14 20:45:16
tags:
- ISLR
- Statistical Learning
- Prediction
- Inference
categories:
- ISLR
mathjax: true
---

## ISLR (Introduction to Statistical Learning)

### Chapter 2. 통계학습

#### 2.1 통계학습이란?

Advertising 자료는 200개의 다른 시장에서 제품의 sales(판매 수치)와 각 시장별로 그 제품에 대한 광고예산으로 구성되어 있다. 광고예산은 TV, radio, newspaper 에 대한 것이다.

![](https://user-images.githubusercontent.com/17154958/68827885-ec9a4780-06e6-11ea-9252-d6bf2f28b03c.png)

- 각 그래프는 각 변수에 대한 sales의 **단순최소제곱적합(simple least squares fit)**을 보여준다. 다시 말하면, 각 그래프의 파란색 직선은 TV, radio, newspaper 각각을 사용하여 sales를 예측하는 데 사용될 수 있는 간단한 모델을 나타낸다.

우리의 고객이 직접적으로 제품의 판매를 증가시킬 수 있는 방법은 없다. 하지만 그는 세 매체에 대한 광고 지출을 제어할 수 있다. 그러므로, 만약 우리가 광고와 판매 사이의 상관관계를 결정할 수 있다면, 우리는 고객에게 광고예산을 조절하게 하여 간접적으로 판매를 증진시킬 수 있다. 다시 말하면, 우리의 목적은 **세 매체에 대한 광고예산을 기반으로 판매를 예측할 수 있는 정확한 모델을 개발하는 것**이다.
여기서, 광고예산은 `입력변수`이고 sales는 `출력변수`이다.

- `input variable` - $X$
  - `predictors` (예측변수)
  - `independent variables` (독립 변수)
  - `features` (특징)
  - `variables` (변수)
- `output variable` - $Y$
  - `response` (반응변수, 응답변수)
  - `dependent variables` (종속변수)

좀 더 일반적으로, 양적(quantitative) 반응변수 $Y$ 개와 $p$ 개의 다른 설명변수, $X_1, X_2, \cdots, X_p$ 가 관찰된다고 해보자. $Y$ 와 $X = (X_1, X_2, \cdots, X_p)$ 사이에 어떤 상관관계가 있다고 가정하면 다음과 같은 일반적인 형태로 나타낼 수 있다.
$$
Y = f(X) + \epsilon
$$
여기서, $f$ 는 $X_1, \cdots, X_p$ 에 대한 알려지지 않은 어떤 고정 함수이고, $\epsilon$ 은 랜덤 오차항(error term)이다.
오차항은 $X$와 독립적이며 평균은 0이다. 이 식에서 $f$는 $X$가 $Y$에 대해 제공하는 체계적인 정보를 나타낸다.

![](https://user-images.githubusercontent.com/17154958/68848196-4284e480-0713-11ea-9747-5d10198fcde4.png)

- 이 그래프는 교육기간을 이용하여 income을 예측할 수도 있음을 시사한다. 하지만, 일반적으로 입력 변수를 출력 변수에 연결하는 함수 $f$는 알려져 있지 않다. 이러한 경우, 함수 $f$ 는 관찰된 점들을 기반으로 추정해야 한다. income은 모의 자료이므로, $f$ 는 알려져 있고 오른쪽 패널에 파란색으로 표시된다. 수직선은 오차항 $\epsilon$ 을 나타낸다. 30개의 관찰치 일부는 파란색 곡선의 윗 부분에 있고 일부 다른 데이터는 곡선 아래에 있다. 전체적으로 오차의 평균은 대략 0이다.

![](https://user-images.githubusercontent.com/17154958/68848041-f5a10e00-0712-11ea-88f0-584abdf45e15.png)

- 위의 그래프에서 $f$는 관찰된 데이터에 기초하여 추정되어야 하는 2차원 곡면(surface)이다.

**본질적으로, 통계학습은 $f$를 추정하는 일련의 기법들을 말하는 것이다.**

#### 2.1.1 $f$ 를 추정하는 이유는?

$f$를 추정하고자 하는 두 가지 주요한 이유는 **예측**과 **추론**이다.

##### 예측 (Prediction)

많은 경우, 입력 $X$는 쉽게 얻을 수 있지만 출력 $Y$는 쉽게 얻을 수 없다. 여기서 오차항은 평균이 영이므로 다음 식을 사용하여 $Y$를 예측할 수 있다.
$$
\hat{Y} = \hat{f}(X)
$$
여기서, $\hat{f}$ 는 $f$ 에 대한 추정을 나타내고 $\hat{Y}$ 는 $Y$ 에 대한 예측 결과를 나타낸다. 이러한 설정에서 $\hat{f}$ 는 보통 `블랙박스(black box)`로 취급된다. 이유는 $\hat{f}$ 가 $Y$에 대한 정확한 예측을 제공한다면 그것의 정확한 형태에 대해서는 통상 신경쓰지 않기 때문이다.
$Y$에 대한 예측인 $\hat{Y}$ 의 정확성은 `축소가능 오차(reducible error)`와 `축소불가능 오차(irreducible error)` 라고 불리는 두 가지에 달려 있다. 일반적으로, $\hat{f}$ 는 $f$ 를 완벽하게 추정하지 못하며, 이러한 부정확성으로 인해 오차가 발생될 것이다. 이러한 오차는 `축소가능`하다. 왜냐하면 가장 적절한 통계학습기법을 사용하여 **$f$를 추정함으로써 $\hat{f}$ 의 정확성을 개선할 수 있기 때문**이다. 하지만, 심지어 $f$를 완벽하게 추정할 수 있어 추정된 반응변수 값이 $\hat{Y} = f(X)$ 의 형태를 취하더라도 예측한 값은 여전히 어떤 오차를 가지고 있을 수 있다. 이러한 이유는 **$Y$도 또한 $\epsilon$ 의 함수이고, 정의에 의하면 $\epsilon$ 은 $X$를 사용하여 예측할 수 없기 때문**이다. 그러므로 $\epsilon$ 과 관련된 변동성도 또한 예측의 정확성에 영향을 미친다. 이것은 `축소불가능` 오차로 알려져 있으며, 이유는 아무리 $f$를 잘 추정하더라도 $\epsilon$ 에 의해 도입된 오차를 줄일 수 없기 때문이다.

축소가능 오차가 0보다 큰 이유: $\epsilon$ 은 $Y$를 **예측하는 데 유용한 측정되지 않은 변수를 포함**할 수 있다. 이 변수들은 측정하지 않으므로 $f$는 예측에 이들을 사용할 수 없다. $\epsilon$ 은 또한 **측정할 수 없는 변동성을 포함**할 수 있다. 예를 들어, 부작용의 위험성은 주어진 날 주어진 환자에 따라 다를 수 있고, 약물 자체의 제조상의 차이 또는 그 환자의 그 날 기분이나 상황에 따라 다를 수 있다.
$$
\begin{align}
E(Y-\hat{Y})^2 
& = E[f(X) + \epsilon - \hat{f}(X)]^2 \\
& = \underbrace{[f(X) - \hat{f}(X)]^2}_{\text{reducible}} + \underbrace{Var(\epsilon)}_{\text{irreducible}}
\end{align}
$$
여기서, $E(Y-\hat{Y})^2$ 은 $Y$의 예측 및 실제값 사이의 차이의 제곱에 대한 평균 또는 기대값(expected value)을 나타내며, $Var(\epsilon)$은 오차항 $\epsilon$ 과 관련된 분산(variable)을 나타낸다.
축소불가능 오차는 항상 $Y$에 대한 예측 정확도의 상한선이 될 것이지만 그 경계는 현실적으로 거의 언제나 알려져있지 않다.

##### 추론 (Inference)

보통 $X_1, \cdots, X_p$ 가 변함에 따라 $Y$가 어떻게 영향을 받는지 이해하는 데 관심이 있다. 이러한 상황에서, $f$를 추정하고자 하지만 반드시 $Y$에 대해 예측하는 것이 목적인 것은 아니다. 대신에 **$X$ 와 $Y$ 사이의 관계를 이해** 하길 원하거나, 좀 더 상세하게는 $X_1, \cdots, X_p$ **의 함수로서 $Y$가 어떻게 변하는지 이해**하고자 한다. 이제 $\hat{f}$ 는 블랙박스로 취급될 수 없다. 왜냐하면, 그것의 **정확한 형태를 알아야 할 필요**가 있기 때문이다.

- 어떤 설명변수들이 반응변수와 관련되어 있는가?
- 반응변수와 각 설명변수 사이의 상관관계는 무엇인가?
- $Y$와 각 설명변수의 상관관계는 선형 방정식을 사용하여 충분히 요약될 수 있는가? 또는 이 상관관계는 더 복잡한가?
  역사적으로 $f$를 추정하는 대부분의 방법들은 선형 형태를 취한다. 어떤 경우에는 이러한 가정이 합리적이거나 심지어 바람직하다. 그러나, **실제 상관관계는 보통 더 복잡하며 선형모델은 입력과 출력변수들 사이의 상관관계를 정확하게 표현하지 못할 수 있다.**

**최종 목적이 예측, 추론 또는 이 둘을 결합한 것인지의 여부에 따라 $f$를 추정하는 데 다른 방법들을 사용하는 것이 적절할 수 있다. 예를 들어, 선형모델들은 비교적 간단하고 해석 가능한 추론을 할 수 있지만 몇몇 다른 기법들만큼 정확한 예측을 할 수 없을 수 있다. 반대로, 몇가지 고도의 비선형적인 기법들은 잠재적으로 $Y$에 대해 아주 정확한 예측을 제공할 수 있지만 추론을 더욱 어렵게 만드는 이해하기 어려운 모델을 초래한다.**

